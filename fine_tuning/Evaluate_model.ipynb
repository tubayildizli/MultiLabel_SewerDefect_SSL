{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation for Sewer Defect Classification\n",
    "\n",
    "This notebook implements comprehensive evaluation procedures for the FINE-TUNED sewer defect classification model, including confusion matrices and specialized metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Library Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import wandb\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = f\"{self.img_dir}/{self.annotations.iloc[idx, 0]}\"\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        labels = torch.tensor(self.annotations.iloc[idx, 1:].astype('float32').values)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.523, 0.453, 0.345], std=[0.210, 0.199, 0.154])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(csv_file='{YOUR_PROJECT_ROOT}/data/fine_tuning/annotations/test/test_labels.csv', \n",
    "                            img_dir='{YOUR_PROJECT_ROOT}/data/fine_tuning/images/test', transform=inference_transform)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def evaluate_model(model, dataloader, threshold=0.5, save_images=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    images = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_outputs.append(outputs.cpu())\n",
    "            if save_images:\n",
    "                images.append(inputs.cpu())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "\n",
    "    # Apply sigmoid and thresholding\n",
    "    all_outputs = torch.sigmoid(all_outputs)\n",
    "    all_outputs = (all_outputs > threshold).float()\n",
    "\n",
    "    # Derive \"ND\" (no defect) as 18th class\n",
    "    labels_nd = (all_labels.sum(dim=1) == 0).float().unsqueeze(1)\n",
    "    outputs_nd = (all_outputs.sum(dim=1) == 0).float().unsqueeze(1)\n",
    "\n",
    "    # Extend label and output tensors\n",
    "    labels_ext = torch.cat([all_labels, labels_nd], dim=1)\n",
    "    outputs_ext = torch.cat([all_outputs, outputs_nd], dim=1)\n",
    "\n",
    "    # Convert to numpy\n",
    "    y_true = labels_ext.numpy()\n",
    "    y_pred = outputs_ext.numpy()\n",
    "\n",
    "    # Compute per-class metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None, zero_division=0)\n",
    "    overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(y_true, y_pred, average='micro', zero_division=0)\n",
    "\n",
    "    for i in range(len(precision)):\n",
    "        class_label = f'Class {i}' if i < labels_ext.shape[1] - 1 else 'Class 17 (ND)'\n",
    "        print(f'{class_label} - Precision: {precision[i]:.4f}, Recall: {recall[i]:.4f}, F1 Score: {f1[i]:.4f}')\n",
    "\n",
    "    print(f'\\nOverall - Precision: {overall_precision:.4f}, Recall: {overall_recall:.4f}, F1 Score: {overall_f1:.4f}')\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Loading and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the model structure (same as during training)\n",
    "model = models.resnet101(weights=None)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = torch.nn.Sequential(\n",
    "    torch.nn.Linear(num_ftrs, 17),\n",
    "    # torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(\"{YOUR_PROJECT_ROOT}/checkpoint/fine_tuning/{FINE_TUNED_MODEL_WEIGHT}\", map_location=device))\n",
    "\n",
    "# Move model to GPU if available\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Advanced Metric Analysis\n",
    "- Class Importance Weighted (CIW) scores\n",
    "- Per-class F2 scores\n",
    "- Normal class F1 score (ND)\n",
    "- Mean Average Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import evaluation\n",
    "\n",
    "# Example: Replace with the actual CIW weights from the Sewer-ML paper\n",
    "# These should sum to 1 and have length 18 (17 classes + ND)\n",
    "ciw_weights = np.array([\n",
    "    1.0000,  # RB\n",
    "    0.5518,  # OB\n",
    "    0.2896,  # PF\n",
    "    0.1622,  # DE\n",
    "    0.6419,  # FS\n",
    "    0.1847,  # IS\n",
    "    0.3559,  # RO\n",
    "    0.3131,  # IN\n",
    "    0.0811,  # AF\n",
    "    0.2275,  # BE\n",
    "    0.2477,  # FO\n",
    "    0.0901,  # GR\n",
    "    0.4167,  # PH\n",
    "    0.4167,  # PB\n",
    "    0.9009,  # OS\n",
    "    0.3829,  # OP\n",
    "    0.4396   # OK\n",
    "])\n",
    "\n",
    "# y_true and y_pred should be numpy arrays, shape (num_samples, 18)\n",
    "# y_pred can be logits or probabilities; use logits if using BCEWithLogitsLoss\n",
    "\n",
    "# If your model outputs are logits, use them directly; if probabilities, use np.logit if needed\n",
    "# Here, assume you have already thresholded your predictions at 0.5\n",
    "\n",
    "# Example: y_pred = model outputs after sigmoid and thresholding\n",
    "# y_true = ground truth labels\n",
    "\n",
    "# If you have torch tensors, convert to numpy:\n",
    "# y_true = y_true_tensor.numpy()\n",
    "# y_pred = y_pred_tensor.numpy()\n",
    "y_true_defects = y_true[:, :17]\n",
    "y_pred_defects = y_pred[:, :17]\n",
    "\n",
    "\n",
    "new_metrics, main_metrics, aux_metrics = evaluation(y_pred_defects, y_true_defects, ciw_weights, threshold=0.5)\n",
    "f1_normal = new_metrics[\"F1_Normal\"]\n",
    "print(\"F1-score for ND (Normal):\", f1_normal)\n",
    "print(\"Main metrics:\", main_metrics)\n",
    "print(\"Class-weighted F2 (CIW-F2):\", new_metrics[\"F2\"])\n",
    "print(\"Per-class F2:\", new_metrics[\"F2_class\"])\n",
    "print(\"Macro F1:\", main_metrics[\"MF1\"])\n",
    "print(\"Micro F1:\", main_metrics[\"mF1\"])\n",
    "print(\"Mean Average Precision (mAP):\", main_metrics[\"mAP\"])\n",
    "print(\"Exact Match Accuracy:\", main_metrics[\"EMAcc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Confusion Matrix Visualization\n",
    "- Focused TP/FP/FN view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def create_single_large_confusion_matrix(y_true, y_pred, class_names, include_nd=True):\n",
    "    \"\"\"Creates confusion matrices for multi-label classification.\"\"\"\n",
    "\n",
    "    if include_nd:\n",
    "        y_true_nd = (y_true.sum(axis=1) == 0).astype(int).reshape(-1, 1)\n",
    "        y_pred_nd = (y_pred.sum(axis=1) == 0).astype(int).reshape(-1, 1)\n",
    "        \n",
    "        y_true_extended = np.concatenate([y_true, y_true_nd], axis=1)\n",
    "        y_pred_extended = np.concatenate([y_pred, y_pred_nd], axis=1)\n",
    "        class_names_extended = class_names + ['ND (No Defect)']\n",
    "    else:\n",
    "        y_true_extended = y_true\n",
    "        y_pred_extended = y_pred\n",
    "        class_names_extended = class_names\n",
    "    \n",
    "    n_classes = y_true_extended.shape[1]\n",
    "    n_samples = y_true_extended.shape[0]\n",
    "    \n",
    "    confusion_matrices = []\n",
    "    for i in range(n_classes):\n",
    "        cm = confusion_matrix(y_true_extended[:, i], y_pred_extended[:, i])\n",
    "        if cm.shape == (1, 1):\n",
    "            if y_true_extended[:, i].sum() == 0:  \n",
    "                cm = np.array([[cm[0, 0], 0], [0, 0]])\n",
    "            else:  \n",
    "                cm = np.array([[0, 0], [0, cm[0, 0]]])\n",
    "        confusion_matrices.append(cm)\n",
    "    \n",
    "    return confusion_matrices, class_names_extended\n",
    "\n",
    "def plot_single_large_confusion_matrix(y_true, y_pred, class_names, include_nd=True, figsize=(25, 20)):\n",
    "    \"\"\"Plots confusion matrix for each class.\"\"\"\n",
    "    \n",
    "    confusion_matrices, class_names_extended = create_single_large_confusion_matrix(\n",
    "        y_true, y_pred, class_names, include_nd\n",
    "    )\n",
    "    \n",
    "    n_classes = len(class_names_extended)\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 6, figsize=figsize) \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (cm, class_name) in enumerate(zip(confusion_matrices, class_names_extended)):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                       xticklabels=['Pred: No', 'Pred: Yes'],\n",
    "                       yticklabels=['True: No', 'True: Yes'],\n",
    "                       cbar=False)\n",
    "            \n",
    "            tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
    "            \n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            ax.set_title(f'{class_name}\\nP:{precision:.3f} R:{recall:.3f} F1:{f1:.3f}', \n",
    "                        fontsize=10, pad=10)\n",
    "            ax.set_xlabel('Predicted', fontsize=8)\n",
    "            ax.set_ylabel('Actual', fontsize=8)\n",
    "            ax.tick_params(labelsize=8)\n",
    "    \n",
    "    # Kullanılmayan subplotları gizle\n",
    "    for i in range(len(class_names_extended), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle('Multi-Label Confusion Matrix - All Classes', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Özet istatistikler tablosu\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"MULTI-LABEL CONFUSION MATRIX SUMMARY\")\n",
    "    print(\"=\"*100)\n",
    "    print(f\"{'Class':<20} {'TN':<8} {'FP':<8} {'FN':<8} {'TP':<8} {'Precision':<10} {'Recall':<10} {'F1-Score':<10}\")\n",
    "    print(\"-\"*100)\n",
    "    \n",
    "    total_tp = total_fp = total_fn = total_tn = 0\n",
    "    \n",
    "    for i, (cm, class_name) in enumerate(zip(confusion_matrices, class_names_extended)):\n",
    "        tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        print(f\"{class_name:<20} {tn:<8} {fp:<8} {fn:<8} {tp:<8} {precision:<10.4f} {recall:<10.4f} {f1:<10.4f}\")\n",
    "        \n",
    "        total_tp += tp\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "        total_tn += tn\n",
    "    \n",
    "    # Genel metrikler\n",
    "    overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    overall_f1 = 2 * (overall_precision * overall_recall) / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "    \n",
    "    print(\"-\"*100)\n",
    "    print(f\"{'OVERALL':<20} {total_tn:<8} {total_fp:<8} {total_fn:<8} {total_tp:<8} {overall_precision:<10.4f} {overall_recall:<10.4f} {overall_f1:<10.4f}\")\n",
    "    print(\"=\"*100)\n",
    "\n",
    "\n",
    "# Defect classes\n",
    "class_names = [\n",
    "    'RB', 'OB', 'PF', 'DE', 'FS', 'IS', 'RO', 'IN', \n",
    "    'AF', 'BE', 'FO', 'GR', 'PH', 'PB', 'OS', 'OP', 'OK'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tek büyük confusion matrix oluştur ve çiz\n",
    "# ND sınıfı dahil (include_nd=True) veya hariç (include_nd=False)\n",
    "plot_single_large_confusion_matrix(y_true[:, :17], y_pred[:, :17], class_names, include_nd=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Error Analysis Export\n",
    "- Export TP/FP/FN cases to CSV\n",
    "- Include filenames for manual inspection\n",
    "- Categorize errors by defect type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tp_fp_fn_csv(y_true, y_pred, dataset, class_names, output_path, include_nd=True):\n",
    "    \"\"\"Export per-class TP, FP, FN filenames as a CSV.\"\"\"\n",
    "    if include_nd:\n",
    "        class_names_extended = class_names + ['ND (No Defect)']\n",
    "    else:\n",
    "        class_names_extended = class_names\n",
    "        y_true = y_true[:, :len(class_names)]\n",
    "        y_pred = y_pred[:, :len(class_names)]\n",
    "    filenames = dataset.annotations.iloc[:, 0].tolist()\n",
    "    records = []\n",
    "    for idx, cls_name in enumerate(class_names_extended):\n",
    "        true_col = y_true[:, idx]\n",
    "        pred_col = y_pred[:, idx]\n",
    "        tp_indices = np.where((true_col == 1) & (pred_col == 1))[0]\n",
    "        fp_indices = np.where((true_col == 0) & (pred_col == 1))[0]\n",
    "        fn_indices = np.where((true_col == 1) & (pred_col == 0))[0]\n",
    "        for i in tp_indices:\n",
    "            records.append({'class': cls_name, 'filename': filenames[i], 'category': 'True Positive'})\n",
    "        for i in fp_indices:\n",
    "            records.append({'class': cls_name, 'filename': filenames[i], 'category': 'False Positive'})\n",
    "        for i in fn_indices:\n",
    "            records.append({'class': cls_name, 'filename': filenames[i], 'category': 'False Negative'})\n",
    "    df = pd.DataFrame(records)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "csv_path = '/workspace/finetuning/results/tp_fp_fn_examples.csv'\n",
    "tp_fp_fn_df = export_tp_fp_fn_csv(y_true, y_pred, test_dataset, class_names, csv_path, include_nd=True)\n",
    "print(f\"Saved TP/FP/FN details to {csv_path}\")\n",
    "tp_fp_fn_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
